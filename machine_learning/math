一. 梯 度 下 降 法 (gradient_descent)


    梯度下降法(learning rate)

    三种梯度下降方法:

        <1> 随机梯度下降

        <2> 批梯度下降

        <3> 小批梯度下降

        learning rate 学习速度增大,会怎样

        learning rate 学习速度减小,会怎样

        在学习一开始因参数离最优解隔得较远,需要保持一个较大的学习率,
        尽快逼近最优解.但是学习到后面的时候,参数和最优解已经隔的比较近,
        还保持最初的学习率,容易越过最优点,在最优点附近来回震荡,
        通俗说,就容易学过头了,跑偏了.



二. 岭 回 归 (ridge_regression)


    岭回归(正则化,惩罚,第二范数)

    目 的 : 防 止 过 拟 合

    参 数 : lambda

            lambda 增大会怎样

            lambda 减小会怎样
